# Cloud Humans Take-Home (Backend + Frontend)

## Aviso importante

**N√£o usei IA para resolver o take-home** (backend/core do desafio). A implementa√ß√£o do endpoint RAG, das features principais e de todas as decis√µes t√©cnicas (estrutura, tecnologias, responsividade, funcionalidades) foram feitas por mim.

Usei assist√™ncia de IA apenas para:
- Organiza√ß√£o e escrita da documenta√ß√£o (este README).
- Acelera√ß√£o no desenvolvimento da UI (frontend), mas mantendo controle total sobre todas as decis√µes de design, estrutura e funcionalidades.

**Por n√£o ter usado IA para o take-home, todos os pontos ruins do c√≥digo (bugs, m√°s pr√°ticas, decis√µes question√°veis) s√£o de minha responsabilidade, assim como todos os m√©ritos (qualidade, boas decis√µes, implementa√ß√µes bem feitas).**

---

## O que √© este projeto?

Este reposit√≥rio implementa o endpoint do desafio Cloud Humans usando **RAG (Retrieval Augmented Generation)**:

- Gera embedding do texto do usu√°rio (OpenAI).
- Faz busca vetorial no **Azure AI Search** (IDS j√° populado).
- Envia ‚Äúcontexto recuperado‚Äù + pergunta para o modelo (OpenAI) e obt√©m uma resposta **restrita ao contexto**.

Al√©m do backend, tamb√©m inclui um **frontend completo** para testar a API, visualizar o contexto recuperado (RAG) e medir o tempo de cada requisi√ß√£o.

---

## Estrutura do reposit√≥rio

- `api/`: NestJS (backend)
- `frontend/`: React + Vite (interface de testes)
- `docker-compose.yml`: sobe backend + frontend + Redis

---

## üì± Como usar a interface

A aplica√ß√£o oferece uma interface completa para interagir com o assistente virtual:

### ‚ú® Funcionalidades principais

- **Perguntas de demonstra√ß√£o**: Ao abrir a aplica√ß√£o, voc√™ ver√° uma sele√ß√£o de perguntas pr√©-configuradas que pode clicar para testar rapidamente o sistema.

- **Tempo de resposta e inten√ß√£o**: Abaixo de cada mensagem do bot, voc√™ ver√°:
  - O tempo que a requisi√ß√£o levou para ser processada (permitindo verificar a performance do cache Redis)
  - Uma badge indicando a inten√ß√£o da mensagem:
    - **Answer**: O bot respondeu com confian√ßa usando o contexto dispon√≠vel
    - **Clarification**: O bot precisa de mais informa√ß√µes do usu√°rio
    - **Escalate**: O bot n√£o pode ajudar e est√° solicitando transfer√™ncia para um humano

- **Contexto recuperado (RAG)**: Do lado direito da tela (ou em um drawer no mobile), voc√™ tem acesso ao contexto recuperado pelo sistema RAG para a mensagem mais recente, incluindo:
  - Trechos de texto relevantes da base de conhecimento
  - Score de relev√¢ncia de cada trecho
  - Tipo de se√ß√£o (FAQ, Manual, etc.)

- **Handover para humano**: O bot pode identificar quando n√£o consegue responder adequadamente e solicitar que a conversa seja transferida para um assistente humano.

- **Interface responsiva**: A aplica√ß√£o √© totalmente responsiva e funciona perfeitamente em todos os tamanhos de tela (smartphones, tablets, desktop).

### ‚ö° Testando a performance do cache Redis

Para visualizar o ganho de performance do cache de embeddings:

1. Envie qualquer mensagem para o bot
2. Observe o tempo de resposta exibido abaixo da mensagem
3. Envie **exatamente a mesma mensagem** novamente
4. Compare os tempos: a segunda requisi√ß√£o ser√° significativamente mais r√°pida

**Observe nas imagens abaixo a diferen√ßa entre o tempo do primeiro request (sem cache) e o segundo (com o embedding j√° em cache):**



### üß™ Testando o handover para humano

Para testar a funcionalidade de handover, voc√™ pode seguir esta sequ√™ncia de mensagens:

1. **Primeira mensagem**: "How much?"
   - O bot pedir√° clarifica√ß√£o sobre qual modelo voc√™ est√° perguntando

2. **Segunda mensagem**: "The car"
   - O bot pedir√° novamente para especificar qual modelo de carro

3. **Terceira mensagem**: "Just tell me"
   - Ap√≥s m√∫ltiplas tentativas de clarifica√ß√£o sem sucesso, o bot solicitar√° transfer√™ncia para um humano

Esta sequ√™ncia demonstra como o sistema identifica quando o usu√°rio n√£o est√° fornecendo informa√ß√µes suficientes mesmo ap√≥s v√°rias tentativas de clarifica√ß√£o, acionando automaticamente o handover.



---

## Como rodar (recomendado: Docker)

### Pr√©-requisitos

Antes de come√ßar, certifique-se de ter instalado:

- **Docker** (vers√£o 20.10 ou superior)
- **Docker Compose** (vers√£o 2.0 ou superior)

Para verificar se est√£o instalados:

```bash
docker --version
docker compose version
```

### Passo 1: Clonar o reposit√≥rio (se ainda n√£o tiver)

```bash
git clone <url-do-repositorio>
cd cloudhumans-takehome
```

### Passo 2: Configurar vari√°veis de ambiente

1. Na raiz do reposit√≥rio, crie um arquivo `.env`:

```bash
# No Windows (PowerShell)
New-Item -Path .env -ItemType File

# No Linux/Mac
touch .env
```

2. Abra o arquivo `.env` e adicione as seguintes vari√°veis:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=sk-sua-chave-openai-aqui

# Azure AI Search Configuration
AZURE_AI_SEARCH_KEY=sua-chave-azure-search-aqui
AZURE_AI_SEARCH_ENDPOINT=https://claudia-db.search.windows.net

# Redis Configuration (j√° configurado no docker-compose.yml)
REDIS_URL=redis://redis:6379
```

**Importante**: 
- Substitua os valores de exemplo pelas suas credenciais reais.
- O `REDIS_URL` j√° est√° configurado no `docker-compose.yml` para comunica√ß√£o entre containers, ent√£o voc√™ pode deixar como est√° ou omitir essa linha.
- **Frontend n√£o precisa de `.env`**: O frontend j√° tem a URL da API configurada por padr√£o (`http://localhost:3000`). S√≥ crie um `frontend/.env` se precisar usar uma porta diferente (vari√°vel: `VITE_API_URL`).

### Passo 3: Construir e iniciar os containers

Execute o comando abaixo na raiz do reposit√≥rio:

```bash
docker-compose up --build
```

**O que acontece:**
- O Docker Compose constr√≥i as imagens do backend e frontend (multi-stage builds).
- Inicia o container do Redis primeiro (com health check).
- Aguarda o Redis ficar saud√°vel antes de iniciar o backend.
- Inicia o backend (NestJS) na porta 3000.
- Aguarda o backend ficar saud√°vel antes de iniciar o frontend.
- Inicia o frontend (Nginx servindo React) na porta 5173.

**Primeira execu√ß√£o pode demorar alguns minutos** (download de imagens base, instala√ß√£o de depend√™ncias, build).

### Passo 4: Verificar se est√° funcionando

1. **Verifique os logs**: voc√™ deve ver mensagens indicando que os servi√ßos est√£o rodando:
   - Redis: `Ready to accept connections`
   - Backend: `Nest application successfully started`
   - Frontend: logs do Nginx (geralmente silenciosos)

2. **Acesse o frontend**: abra seu navegador em `http://localhost:5173`
   - Voc√™ deve ver a interface do chat.

3. **Teste a API diretamente** (opcional): fa√ßa uma requisi√ß√£o para `http://localhost:3000/conversations/completions` usando Postman, curl ou similar.

### Passo 5: Parar os containers

Para parar todos os servi√ßos:

```bash
docker-compose down
```

Para parar e remover volumes (limpar dados do Redis):

```bash
docker-compose down -v
```

### Troubleshooting

- **Erro de porta em uso**: se as portas 3000 ou 5173 estiverem ocupadas, altere no `docker-compose.yml` ou pare o processo que est√° usando a porta.
- **Erro de vari√°veis de ambiente**: verifique se o arquivo `.env` est√° na raiz e se todas as vari√°veis est√£o preenchidas corretamente.
- **Container n√£o inicia**: execute `docker-compose logs <nome-do-servico>` (ex.: `docker-compose logs backend`) para ver os logs de erro.
- **Build falha**: certifique-se de ter espa√ßo em disco suficiente e conex√£o com a internet (para download de imagens e pacotes).

---

## Rodando localmente (sem Docker)

Esta op√ß√£o √© √∫til para desenvolvimento, j√° que permite hot-reload e debug mais f√°cil.

### Pr√©-requisitos

- **Node.js** (vers√£o 20 ou superior)
- **pnpm** (gerenciador de pacotes)
- **Redis** (para cache de embeddings)

Para instalar o pnpm (se n√£o tiver):

```bash
npm install -g pnpm
```

Para instalar o Redis:

- **Windows**: use WSL2 ou instale via [Redis for Windows](https://github.com/microsoftarchive/redis/releases) ou [Docker Desktop](https://www.docker.com/products/docker-desktop) (apenas Redis).
- **Linux**: `sudo apt-get install redis-server` (Ubuntu/Debian) ou equivalente.
- **Mac**: `brew install redis`

### Passo 1: Configurar Redis local

1. **Inicie o Redis**:

```bash
# Windows (WSL ou via Docker)
redis-server

# Linux/Mac
redis-server
# ou, se instalado via Homebrew no Mac:
brew services start redis
```

2. **Verifique se est√° rodando**:

```bash
redis-cli ping
# Deve retornar: PONG
```

O Redis estar√° dispon√≠vel em `redis://localhost:6379` (padr√£o).

### Passo 2: Configurar vari√°veis de ambiente do backend

1. Na pasta `api/`, crie um arquivo `.env`:

```bash
cd api
# Windows (PowerShell)
New-Item -Path .env -ItemType File

# Linux/Mac
touch .env
```

2. Abra o arquivo `api/.env` e adicione:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=sk-sua-chave-openai-aqui

# Azure AI Search Configuration
AZURE_AI_SEARCH_KEY=sua-chave-azure-search-aqui
AZURE_AI_SEARCH_ENDPOINT=https://claudia-db.search.windows.net

# Redis Configuration (local)
REDIS_URL=redis://localhost:6379

# Node Environment
NODE_ENV=development
```

### Passo 3: Instalar depend√™ncias e rodar o backend

1. **Instale as depend√™ncias**:

```bash
cd api
pnpm install
```

2. **Inicie o servidor em modo desenvolvimento**:

```bash
pnpm start:dev
```

**O que acontece:**
- O NestJS compila o c√≥digo TypeScript.
- Inicia o servidor na porta 3000 (padr√£o).
- Habilita hot-reload (recarrega automaticamente ao salvar arquivos).

Voc√™ deve ver no terminal:
```
[Nest] INFO [NestFactory] Starting Nest application...
[Nest] INFO [InstanceLoader] ...
[Nest] INFO [NestFactory] Nest application successfully started on http://[::1]:3000
```

3. **Teste o backend** (opcional): acesse `http://localhost:3000` no navegador ou fa√ßa uma requisi√ß√£o para `http://localhost:3000/conversations/completions`.

### Passo 4: Configurar e rodar o frontend

1. **Instale as depend√™ncias**:

```bash
cd frontend
pnpm install
```

2. **Configure a URL da API** (opcional):

   - **Por padr√£o, o frontend j√° est√° configurado para `http://localhost:3000`**.
   - **Voc√™ N√ÉO precisa criar um `.env`** a menos que queira usar uma porta/URL diferente.
   - Se precisar alterar, crie um arquivo `.env` na pasta `frontend/` com a vari√°vel `VITE_API_URL`:

```bash
# frontend/.env (OPCIONAL - s√≥ se precisar mudar a URL/porta)
VITE_API_URL=http://localhost:3000
```

3. **Inicie o servidor de desenvolvimento**:

```bash
pnpm dev
```

**O que acontece:**
- O Vite compila o React e inicia o servidor de desenvolvimento.
- Geralmente roda na porta 5173 (voc√™ ver√° a URL no terminal).
- Habilita hot-reload (recarrega automaticamente ao salvar arquivos).

Voc√™ deve ver no terminal algo como:
```
  VITE v5.x.x  ready in xxx ms

  ‚ûú  Local:   http://localhost:5173/
  ‚ûú  Network: use --host to expose
```

4. **Acesse o frontend**: abra `http://localhost:5173` no navegador.

### Passo 5: Verificar se est√° tudo funcionando

1. **Backend rodando**: voc√™ deve conseguir acessar `http://localhost:3000` (ou ver logs de requisi√ß√µes no terminal).
2. **Frontend rodando**: voc√™ deve ver a interface do chat em `http://localhost:5173`.
3. **Teste completo**: envie uma mensagem no chat e verifique se:
   - A requisi√ß√£o √© enviada para o backend.
   - A resposta √© exibida no chat.
   - O contexto recuperado (RAG) aparece no painel lateral.

### Ordem de inicializa√ß√£o recomendada

1. Redis (deve estar rodando primeiro)
2. Backend (aguarda Redis estar dispon√≠vel)
3. Frontend (pode iniciar em paralelo, mas precisa do backend para funcionar)

### Troubleshooting

- **Erro "Cannot connect to Redis"**: verifique se o Redis est√° rodando (`redis-cli ping`) e se a `REDIS_URL` est√° correta.
- **Erro de porta em uso**: altere a porta no c√≥digo ou pare o processo que est√° usando a porta.
- **Erro de vari√°veis de ambiente**: verifique se o arquivo `.env` est√° na pasta correta (`api/.env` para backend, `frontend/.env` para frontend se necess√°rio).
- **Erro de depend√™ncias**: delete `node_modules` e `pnpm-lock.yaml`, depois execute `pnpm install` novamente.
- **Frontend n√£o conecta ao backend**: verifique se `VITE_API_URL` est√° correto e se o backend est√° rodando e acess√≠vel.

---

## Endpoint principal

### `POST /conversations/completions`

Exemplo de request:

```json
{
  "helpDeskId": 123456,
  "projectName": "tesla_motors",
  "messages": [
    { "role": "USER", "content": "How long does a Tesla battery last before it needs to be replaced?" }
  ]
}
```

Exemplo de response (resumido):

```json
{
  "messages": [
    { "role": "USER", "content": "..." },
    { "role": "AGENT", "content": "...", "intent": "answer" }
  ],
  "handOverToHumanNeeded": false,
  "sectionsRetrieved": [
    { "score": 0.60, "content": "...", "type": "N1" }
  ]
}
```

---

## Decis√µes t√©cnicas principais

### RAG (Retrieval Augmented Generation)

- **Por que**: responder usando apenas o ‚ÄúImproved Data Set (IDS)‚Äù e evitar alucina√ß√µes.
- **O que traz**: respostas fundamentadas em contexto recuperado + visibilidade do que foi usado (o frontend mostra isso).

### Feature escolhida: Clarification Feature

- **Regra**: quando n√£o houver informa√ß√£o suficiente, o agente pode pedir esclarecimentos.
- **Limite**: at√© 2 clarifica√ß√µes por conversa; se precisar de uma terceira, deve escalar para humano (handover).

---

## Extras que eu adicionei (n√£o exigidos, mas importantes)

### Redis caching (embeddings)

- **Por que eu adicionei**: embeddings s√£o caros e repetitivos (mesmas perguntas aparecem muito em atendimento). Cache reduz custo e melhora lat√™ncia.
- **O que traz**:
  - Menos chamadas ao provider de embeddings.
  - Respostas mais r√°pidas em perguntas repetidas.
  - Uma base simples para evoluir para estrat√©gias como ‚Äúcache por normaliza√ß√£o de texto‚Äù e ‚Äúcache por similaridade‚Äù.

Onde est√°:
- Cache configurado no NestJS em `api/src/app.module.ts` via `CacheModule` + Redis store.
- Uso do cache no fluxo de embeddings em `api/src/openai/openai.service.ts` (cache key por texto).

### Logging interceptor (m√©tricas e observabilidade)

- **Por que eu adicionei**: para troubleshooting e performance, √© essencial enxergar tempo de execu√ß√£o e rota chamada (especialmente em um sistema com IO externo: OpenAI + Azure Search).
- **O que traz**:
  - Logging estruturado por request (m√©todo, URL, dura√ß√£o).
  - Base para adicionar tracing/correlation-id no futuro.

Onde est√°:
- `api/src/common/logging.interceptor.ts`
- Registrado globalmente em `api/src/main.ts`.

### Exception filter (respostas de erro consistentes)

- **Por que eu adicionei**: manter respostas de erro padronizadas ajuda o frontend e facilita depura√ß√£o.
- **O que traz**:
  - Payload de erro consistente (`statusCode`, `message`, `path`, `timestamp`).
  - Log estruturado do erro no servidor.

Onde est√°:
- `api/src/common/filters/all-exceptions.filter.ts`
- Registrado globalmente em `api/src/main.ts`.

### Frontend completo (UI de teste + observabilidade de RAG)

- **Por que eu adicionei**: uma UI acelera revis√£o do desafio, demonstra o fluxo RAG e reduz fric√ß√£o para testes manuais (sem Postman).
- **O que traz**:
  - Interface estilo ‚Äúchat‚Äù para simular conversa real.
  - Painel de contexto (RAG evidence) com score e tipo (`N1`/`N2`).
  - Indicadores de escalonamento/clarifica√ß√£o.
  - **Medi√ß√£o da dura√ß√£o da requisi√ß√£o** por chamada (√∫til para comparar ‚Äúprimeira vez‚Äù vs ‚Äúcache hit‚Äù).

Onde est√°:
- `frontend/src/services/api.ts` (axios + interceptors para medir dura√ß√£o)
- `frontend/src/components/ContextPanel.tsx` / `ContextDrawer.tsx` (visualiza√ß√£o do RAG)

---

## Como evoluir (ideias)

- **Guardrail IDS-only**: validar no backend se a resposta est√° suportada pelo contexto (ex.: exigir cita√ß√£o de trecho / id de se√ß√£o).
- **Cache inteligente**: normaliza√ß√£o do texto (trim/lower/remove punctuation) e TTL por tipo de conte√∫do.
- **Observabilidade**: correlation-id, logs por request incluindo helpDeskId/projectName, exporta√ß√£o para APM.
- **Seguran√ßa**: rate limit, valida√ß√£o mais restrita de payload, e CORS por ambiente.

